#!/usr/bin/python
# -*- encoding: utf-8 -*-
# File    :   subStruFilterCom.py
# Time    :   2025/08/12 16:32:11
# Author  :   Y.ZHANG (UniOfSurrey)
# Email   :   yu.zhang@surrey.ac.uk

import re
import pandas as pd
import sympy as sp
from collections import Counter

def extract_substructures(equation):
    """
    ç²—ç•¥åœ°æå–æ‹¬å·å†…çš„ç»“æ„è¡¨è¾¾å¼ä½œä¸ºå­ç»“æ„ï¼Œ
    ä¹Ÿæå–å¦‚ sqrt(x8), inv(x13 + 5.1) ç­‰æ¨¡å¼ã€‚
    """
    substructures = []

    # æå–æ‹¬å·ç»“æ„
    parens = re.findall(r"\(([^()]+)\)", equation)
    substructures.extend(parens)

    # æå–å‡½æ•°ç»“æ„ï¼Œå¦‚ sqrt(...) / inv(...) / log(...) ç­‰
    funcs = re.findall(r"(sqrt|inv|log|exp)\(([^()]+)\)", equation)
    substructures.extend([f"{f}({s})" for f, s in funcs])

    return substructures

def calc_complexity(expr_str, mode="count_ops"):  #TODO: åº”è¯¥é€‰node_count
    """
    è®¡ç®—è¡¨è¾¾å¼å¤æ‚åº¦
    mode:
        'count_ops'  - æŒ‰è¿ç®—ç¬¦æ•°é‡è®¡ç®—ï¼ˆæ¨èï¼Œå’Œ PySR ä¸€è‡´ï¼‰TODO: å¯èƒ½éœ€è¦è°ƒæ•´. PySRå…¶å®æ˜¯æŒ‰èŠ‚ç‚¹æ•°è®¡ç®—çš„
        'node_count' - æŒ‰è¡¨è¾¾å¼èŠ‚ç‚¹æ€»æ•°è®¡ç®—
    """
    try:
        expr = sp.sympify(expr_str)
    except Exception:
        return 999  # æ— æ³•è§£æç›´æ¥åˆ¤ä¸ºé«˜å¤æ‚åº¦

    if mode == "count_ops":
        return sp.count_ops(expr, visual=False)
    elif mode == "node_count":
        return sum(1 for _ in sp.preorder_traversal(expr))
    else:
        raise ValueError("mode must be 'count_ops' or 'node_count'")

def statistics_for_structure_frequency(df, complexity_mode="count_ops"):
    """ç»Ÿè®¡æ¯ä¸ªå­ç»“æ„åœ¨æ‰€æœ‰è¡¨è¾¾å¼ä¸­çš„å‡ºç°é¢‘ç‡"""
    structure_counter = Counter()
    for eq in df["equation"]:
        subs = extract_substructures(eq)
        structure_counter.update(subs)

    top_structures = [(s, freq) for s, freq in structure_counter.items() if freq > 1]
    top_structures = sorted(top_structures, key=lambda x: x[1], reverse=True)

    print("Top substructures by frequency:")
    for s, freq in top_structures:
        comp = calc_complexity(s, mode=complexity_mode)
        print(f"{s:40} | freq={freq:3d} | complexity={comp}")

    return top_structures

def build_new_feature(top_structures, run, n_febank, max_complexity=6, complexity_mode="count_ops"):  # TODO: æŒ‰ç…§ç›®å‰çš„ç»“æœï¼Œè¿™é‡Œåº”è¯¥é€‰çš„æ˜¯8
    """
    æ ¹æ®ç»“æ„é¢‘ç‡æ„å»ºæ–°çš„ç‰¹å¾ï¼Œå¢åŠ å¤æ‚åº¦(limitation=6, æ§åˆ¶å­ç»“æ„ä¸ä¼šæ— é™å¤æ‚åŒ–)ç­›é€‰
    """
    # è¯»å–å˜é‡ç´¢å¼•æ˜ å°„
    var_map_df = pd.read_csv(f"Symbolic Regression/srloop/data/variable_index_mapping_run-{run}.csv")
    var_index_to_name = dict(zip(var_map_df["Index"], var_map_df["Variable"]))

    new_feature_bank = {}
    for s, _ in top_structures:
        feature_expr = replace_var_indices(s, var_index_to_name)

        # å¤æ‚åº¦ç­›é€‰
        comp = calc_complexity(feature_expr, mode=complexity_mode)
        if comp > max_complexity:
            print(f"[SKIP:Complexity>{max_complexity}] {feature_expr} (complexity={comp})")
            continue
        print(f"[ADD] {feature_expr} (complexity={comp})")

        new_feature_bank[s] = feature_expr

    print(f"âœ… Passed filtering: {len(new_feature_bank)} features.")

    # æ£€æŸ¥å”¯ä¸€æ€§
    existing_features = pd.read_csv(f"Symbolic Regression/srloop/data/raw_feature_index_mapping_run-{run}.csv")
    unique_new_feature_bank = {
        k: v for k, v in new_feature_bank.items()
        if v not in existing_features['Variable'].values
    }

    # è®¡ç®—unique_new_feature_bankçš„å¤æ‚åº¦
    unique_complexities = {k: calc_complexity(v, mode=complexity_mode) for k, v in unique_new_feature_bank.items()}
    max_unique_comp = max(unique_complexities.values()) if unique_complexities else 0

    if unique_new_feature_bank:
        n_febank = True
        new_feature_df = pd.DataFrame(list(unique_new_feature_bank.items()),
                                      columns=["Original Structure", "Feature Expression"])
        new_feature_df.to_csv(f"Symbolic Regression/srloop/data/new_feature_bank_run-{run}.csv", index=False)
        print(f"ğŸ’¾ Saved {len(unique_new_feature_bank)} features to new_feature_bank_run-{run}.csv")
    else:
        n_febank = False
        print("âš  No new unique features to add.")
    

    return unique_new_feature_bank, n_febank, max_unique_comp

def replace_var_indices(expr, var_map):
    """æ›¿æ¢è¡¨è¾¾å¼ä¸­çš„ xN ä¸ºå®é™…å˜é‡å"""
    def repl(m):
        idx = m.group(1)
        return var_map.get(int(idx), f"x{idx}")
    return re.sub(r"x(\d+)", repl, expr)

if __name__ == "__main__":

    # Test the functions here
    test_equation = "inv(C_pg - 10.475055) * C_pg / C_eth * C_pg / C_eth * 1.3445362"
    print("Extracted substructures:", extract_substructures(test_equation))
    print("Complexity of test equation:", calc_complexity(test_equation))

    # Create a dummy DataFrame for testing
    df_test = pd.DataFrame({
        "equation": [
            "sqrt(x1 + x2) * inv(x3)",
            "log(x1) + exp(x2)",
            "x1 * x2 + x3"
        ]
    })
    print("Statistics for structure frequency:")
    statistics_for_structure_frequency(df_test)
